{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7b8b6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import numpy as np  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ba30e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    \"AC.PA\",   # Accor  \n",
    "    \"AI.PA\",   # Air Liquide  \n",
    "    \"AIR.PA\",  # Airbus  \n",
    "    \"CS.PA\",   # Axa  \n",
    "    \"BNP.PA\",  # BNP Paribas  \n",
    "    \"EN.PA\",   # Bouygues  \n",
    "    \"BVI.PA\",  # Bureau Veritas  \n",
    "    \"CAP.PA\",  # Capgemini  \n",
    "    \"CA.PA\",   # Carrefour  \n",
    "    \"ACA.PA\",  # Crédit Agricole  \n",
    "    \"BN.PA\",   # Danone (attention : s'assurer du bon ticker, parfois “BN.PA” peut poser problème selon la source)  \n",
    "    \"DSY.PA\",  # Dassault Systèmes  \n",
    "    \"EDEN.PA\", # Edenred  \n",
    "    \"ENGI.PA\", # Engie  \n",
    "    \"EL.PA\",   # EssilorLuxottica  \n",
    "    \"ERF.PA\",  # Eurofins Scientific  \n",
    "    \"RMS.PA\",  # Hermès (note : sur Yahoo, Hermès peut être “RMS.PA”)  \n",
    "    \"KER.PA\",  # Kering  \n",
    "    \"OR.PA\",   # L’Oréal  \n",
    "    \"LR.PA\",   # Legrand  \n",
    "    \"MC.PA\",   # LVMH  \n",
    "    \"ML.PA\",   # Michelin  \n",
    "    \"ORA.PA\",  # Orange  \n",
    "    \"RI.PA\",   # Pernod Ricard  \n",
    "    \"PUB.PA\",  # Publicis Groupe  \n",
    "    \"RNO.PA\",  # Renault  \n",
    "    \"SAF.PA\",  # Safran  \n",
    "    \"SGO.PA\",  # Saint-Gobain  \n",
    "    \"SAN.PA\",  # Sanofi  \n",
    "    \"SU.PA\",   # Schneider Electric  \n",
    "    \"GLE.PA\",  # Société Générale  \n",
    "    \"STLAP.PA\",# Stellantis  \n",
    "    \"STMPA.PA\",# STMicroelectronics (avec le “.PA”)  \n",
    "    \"TEP.PA\",  # Teleperformance  \n",
    "    \"HO.PA\",   # Thales  \n",
    "    \"TTE.PA\",  # TotalEnergies  \n",
    "    \"URW.PA\",  # Unibail-Rodamco-Westfield  \n",
    "    \"VIE.PA\",  # Veolia Environnement  \n",
    "    \"DG.PA\",   # Vinci  \n",
    "    \"VIV.PA\"   # Vivendi  \n",
    "# ,\"BOVA11.SA\", \"BBDC4.SA\", \"CIEL3.SA\", \"TIUB4.SA\", \"PETR4.SA\"\n",
    "]\n",
    "\n",
    "nb_days = 59\n",
    "force_dl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e0d9ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_max(date1 : dt.datetime,date2 : dt.datetime) -> dt.datetime:\n",
    "    if date1 > date2:\n",
    "        return date1.replace(hour=0, minute=0, second=0)\n",
    "    else:\n",
    "        return date2.replace(hour=0, minute=0, second=0)\n",
    "    \n",
    "def dl_data(company : str, start_date : dt.datetime, end_date : dt.datetime, interval : str) -> pd.DataFrame:\n",
    "    if((end_date-start_date).days > 7):\n",
    "        start_data = dl_data(company ,date_max(start_date, dt.datetime.now() - dt.timedelta(days=nb_days)), start_date + dt.timedelta(days=7), interval)\n",
    "        end_data = dl_data(company ,start_date + dt.timedelta(days=7), end_date, interval)\n",
    "        return pd.concat([start_data, end_data])\n",
    "    else:\n",
    "        data = yf.Ticker(company)\n",
    "        data = data.history(start=start_date, end=end_date, interval=interval)\n",
    "        data.reset_index(inplace=True)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1787ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "\n",
    "datas = []\n",
    "for company in tickers:\n",
    "    if not os.path.exists(\"data/\" + company + \".csv\") or force_dl:\n",
    "        \n",
    "        data = dl_data(company, dt.datetime.now() - dt.timedelta(days=nb_days), dt.datetime.now(), \"15m\")\n",
    "        data = data.drop(columns=[\"Dividends\",\"Stock Splits\"])\n",
    "        # flemme pour le moment    \n",
    "        # try:\n",
    "        #     old_data = pd.read_csv(\"data/\" + company + \".csv\")\n",
    "        # except FileNotFoundError:\n",
    "        #     old_data = pd.DataFrame()\n",
    "            \n",
    "        # # Concatenate old and new data but if the datetime is already in the old data, keep the new one\n",
    "        # data = pd.concat([old_data, data])\n",
    "        # data = data.drop_duplicates(subset=[\"Datetime\"])\n",
    "        data.to_csv(\"data/\" + company + \".csv\")\n",
    "        datas.append(data)\n",
    "    else:\n",
    "        datas.append(pd.read_csv(\"data/\" + company + \".csv\", index_col=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e517f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Datetime       Open       High        Low      Close  \\\n",
      "1  2025-09-22 09:15:00+02:00  41.009998  41.080002  40.910000  40.970001   \n",
      "2  2025-09-22 09:30:00+02:00  40.990002  41.119999  40.990002  41.080002   \n",
      "3  2025-09-22 09:45:00+02:00  41.070000  41.169998  40.990002  41.150002   \n",
      "4  2025-09-22 10:00:00+02:00  41.139999  41.220001  41.139999  41.189999   \n",
      "5  2025-09-22 10:15:00+02:00  41.200001  41.209999  41.139999  41.139999   \n",
      "\n",
      "   Volume  log_return  smooth_close  y  \n",
      "1    8473   -0.001220     41.015001  0  \n",
      "2    6167    0.002681     41.021501  1  \n",
      "3    6415    0.001703     41.034351  1  \n",
      "4    4763    0.000972     41.049916  1  \n",
      "5    4501   -0.001215     41.058924  0  \n"
     ]
    }
   ],
   "source": [
    "lambda_smooth = 0.1\n",
    "\n",
    "span = 2/lambda_smooth - 1\n",
    "for data in datas:\n",
    "    data['log_return'] = np.log(data[\"Close\"]) - np.log(data[\"Close\"].shift(1))\n",
    "    data['smooth_close'] = data[\"Close\"].ewm(span=span, adjust=False).mean()\n",
    "    data['y'] = (data[\"Close\"] > data[\"Close\"].shift(1)).astype(int)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "print(datas[0].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "621b8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = datas[0].columns\n",
    "cols_to_scale = cols_to_scale.delete(cols_to_scale.get_loc(\"Datetime\"))\n",
    "cols_to_scale = cols_to_scale.delete(cols_to_scale.get_loc(\"y\"))\n",
    "\n",
    "df_concat = pd.concat(datas, keys=tickers, names=['Ticker', 'Row'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_concat[cols_to_scale] = scaler.fit_transform(df_concat[cols_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "166045a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datas:\n",
    "    data[cols_to_scale] = scaler.transform(data[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0d1d75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_sequences(df: pd.DataFrame, k: int, interval_minutes: int = 15) -> tuple[list[pd.DataFrame], list[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Crée des séquences continues de taille k+1 (k étapes d'entrée et 1 étape de sortie) à partir du DataFrame.\n",
    "    S'assure que la différence de temps entre chaque étape est exactement de interval_minutes.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant une colonne 'Datetime'.\n",
    "        k (int): Taille de la fenêtre glissante (séquence d'entrée).\n",
    "        interval_minutes (int): Intervalle de temps attendu en minutes (défaut: 15).\n",
    "        \n",
    "    Returns:\n",
    "        list[pd.DataFrame]: Liste de DataFrames contenant les séquences d'entrée et de sortie.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Vérifications\n",
    "    if 'Datetime' not in df.columns:\n",
    "        raise ValueError(\"Le DataFrame doit avoir une colonne 'Datetime'\")\n",
    "        \n",
    "    df = df.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Datetime']):\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True)\n",
    "        \n",
    "    df = df.sort_values('Datetime')\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    \n",
    "    df['time_diff'] = df['Datetime'].diff()\n",
    "    \n",
    "    df['gap'] = df['time_diff'] != pd.Timedelta(minutes=interval_minutes)\n",
    "    \n",
    "    df['group_id'] = df['gap'].cumsum()\n",
    "    \n",
    "    cols_to_keep = [c for c in df.columns if c not in ['time_diff', 'gap', 'group_id']]\n",
    "    \n",
    "    # Pour chaque groupe continu, on extrait les séquences\n",
    "    for g_id, group in df.groupby('group_id'):\n",
    "        group_data = group[cols_to_keep]\n",
    "        \n",
    "        if len(group_data) > k:\n",
    "            # Pour avoir une séquence d'entrée de taille k et une cible à k+1,\n",
    "            for i in range(len(group_data) - k):\n",
    "                seq = group_data.iloc[i : i+k].copy()\n",
    "                seq.drop(columns=['Datetime','y'], inplace=True)\n",
    "                seq = seq.sort_index(axis=1)\n",
    "                target = group_data.iloc[[i+k]].copy()\n",
    "                \n",
    "                sequences.append(seq)\n",
    "                targets.append(target[\"y\"].values[-1]) # strat moldave\n",
    "                \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7e796d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "k = 20\n",
    "time_interval = 15\n",
    "for data in datas:\n",
    "    seqs, targs = create_continuous_sequences(data, k, time_interval)\n",
    "    inputs.extend(seqs)\n",
    "    outputs.extend(targs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "afe6be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[[-3.55099002e-01 -3.54922377e-01 -3.55124963e-01 -3.54982474e-01\n",
      "  -2.96539891e-01 -3.69198996e-01 -3.54937667e-01]\n",
      " [-3.54774183e-01 -3.54804471e-01 -3.54888317e-01 -3.55041522e-01\n",
      "  -3.57843719e-01  8.14348252e-01 -3.54918472e-01]\n",
      " [-3.54567483e-01 -3.54657079e-01 -3.54888317e-01 -3.54805297e-01\n",
      "  -3.51250766e-01  5.17388611e-01 -3.54880526e-01]\n",
      " [-3.54449376e-01 -3.54509676e-01 -3.54444622e-01 -3.54598596e-01\n",
      "  -3.95168340e-01  2.95596256e-01 -3.54834562e-01]\n",
      " [-3.54597018e-01 -3.54539161e-01 -3.54444622e-01 -3.54421419e-01\n",
      "  -4.02133476e-01 -3.67670854e-01 -3.54807960e-01]\n",
      " [-3.54715136e-01 -3.54745512e-01 -3.54710843e-01 -3.54628120e-01\n",
      "  -4.21992088e-01 -2.94302526e-01 -3.54795831e-01]\n",
      " [-3.55069478e-01 -3.54922377e-01 -3.54917903e-01 -3.54775762e-01\n",
      "  -2.45604012e-01 -8.86286212e-01 -3.54820350e-01]\n",
      " [-3.55660057e-01 -3.55158213e-01 -3.55539083e-01 -3.55041522e-01\n",
      "  -2.61847134e-01 -1.48351142e+00 -3.54901479e-01]\n",
      " [-3.55807698e-01 -3.55777265e-01 -3.55834890e-01 -3.55661637e-01\n",
      "  -1.99054574e-01 -3.71378116e-01 -3.54989259e-01]\n",
      " [-3.55837222e-01 -3.55806750e-01 -3.55716567e-01 -3.55779744e-01\n",
      "  -4.24677121e-01 -7.36469656e-02 -3.55071214e-01]\n",
      " [-3.55748639e-01 -3.55865709e-01 -3.55746142e-01 -3.55809279e-01\n",
      "  -3.66377340e-01  2.24275047e-01 -3.55136115e-01]\n",
      " [-3.55305703e-01 -3.55452997e-01 -3.55598244e-01 -3.55750220e-01\n",
      "  -4.04845094e-01  1.11560970e+00 -3.55150230e-01]\n",
      " [-3.55394297e-01 -3.55364564e-01 -3.55243286e-01 -3.55307283e-01\n",
      "  -4.05004601e-01 -2.21803320e-01 -3.55171793e-01]\n",
      " [-3.55364762e-01 -3.55364564e-01 -3.55272861e-01 -3.55366342e-01\n",
      "  -4.36108451e-01  7.50822280e-02 -3.55188246e-01]\n",
      " [-3.55689580e-01 -3.55541441e-01 -3.55539083e-01 -3.55425401e-01\n",
      "  -3.79111309e-01 -8.16653321e-01 -3.55235537e-01]\n",
      " [-3.55748639e-01 -3.55777265e-01 -3.55598244e-01 -3.55720696e-01\n",
      "  -4.12953362e-01 -1.48034554e-01 -3.55284006e-01]\n",
      " [-3.55807698e-01 -3.55747792e-01 -3.55657406e-01 -3.55720696e-01\n",
      "  -3.72412019e-01 -1.48107641e-01 -3.55333534e-01]\n",
      " [-3.55748639e-01 -3.55688833e-01 -3.55686981e-01 -3.55809279e-01\n",
      "  -3.93546686e-01  1.49787935e-01 -3.55372203e-01]\n",
      " [-3.55837222e-01 -3.55865709e-01 -3.55923627e-01 -3.55750220e-01\n",
      "   8.07604999e-04 -2.22594753e-01 -3.55415863e-01]\n",
      " [-3.56162041e-01 -3.56013101e-01 -3.56012364e-01 -3.55868338e-01\n",
      "  -4.17286634e-01 -8.19869822e-01 -3.55487641e-01]]\n",
      "23766\n"
     ]
    }
   ],
   "source": [
    "print(type(inputs[0]))\n",
    "input0 = inputs[0]\n",
    "num_input0 = input0.to_numpy()\n",
    "print(num_input0)\n",
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "86a70eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([df.to_numpy() for df in inputs])\n",
    "outputs = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "98c4b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23766, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "28e77833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20, num_layers=1, output_size=1):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Couche fully connected pour la sortie binaire\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : [batch_size, seq_length, input_size]\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Prendre la dernière sortie de la séquence\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "36550727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 7)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].shape)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4abb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test =  torch.from_numpy(inputs[:int(0.8*len(inputs))]).float(), torch.from_numpy(inputs[int(0.8*len(inputs)):]).float()\n",
    "y_train, y_test = torch.from_numpy(outputs[:int(0.8*len(outputs))]).float().unsqueeze(1), torch.from_numpy(outputs[int(0.8*len(outputs)):]).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "44ae215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparamètres\n",
    "input_size = inputs[0].shape[1]    # Nombre de features (prix + indicateurs techniques)\n",
    "\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "output_size = 1     # Classe binaire\n",
    "seq_length = 10     # Nombre de pas temporels (historique utilisé)\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a684f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialiser le modèle, la loss et l’optimiseur\n",
    "model = StockPriceLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy pour classification binaire\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8517de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.6917\n",
      "Epoch [10/1000], Loss: 0.6916\n",
      "Epoch [20/1000], Loss: 0.6916\n",
      "Epoch [30/1000], Loss: 0.6915\n",
      "Epoch [40/1000], Loss: 0.6915\n",
      "Epoch [50/1000], Loss: 0.6914\n",
      "Epoch [60/1000], Loss: 0.6913\n",
      "Epoch [70/1000], Loss: 0.6912\n",
      "Epoch [80/1000], Loss: 0.6911\n",
      "Epoch [90/1000], Loss: 0.6909\n",
      "Epoch [100/1000], Loss: 0.6907\n",
      "Epoch [110/1000], Loss: 0.6904\n",
      "Epoch [120/1000], Loss: 0.6901\n",
      "Epoch [130/1000], Loss: 0.6898\n",
      "Epoch [140/1000], Loss: 0.6896\n",
      "Epoch [150/1000], Loss: 0.6893\n",
      "Epoch [160/1000], Loss: 0.6889\n",
      "Epoch [170/1000], Loss: 0.6885\n",
      "Epoch [180/1000], Loss: 0.6880\n",
      "Epoch [190/1000], Loss: 0.6876\n",
      "Epoch [200/1000], Loss: 0.6872\n",
      "Epoch [210/1000], Loss: 0.6867\n",
      "Epoch [220/1000], Loss: 0.6862\n",
      "Epoch [230/1000], Loss: 0.6857\n",
      "Epoch [240/1000], Loss: 0.6851\n",
      "Epoch [250/1000], Loss: 0.6845\n",
      "Epoch [260/1000], Loss: 0.6839\n",
      "Epoch [270/1000], Loss: 0.6835\n",
      "Epoch [280/1000], Loss: 0.6829\n",
      "Epoch [290/1000], Loss: 0.6825\n",
      "Epoch [300/1000], Loss: 0.6821\n",
      "Epoch [310/1000], Loss: 0.6816\n",
      "Epoch [320/1000], Loss: 0.6812\n",
      "Epoch [330/1000], Loss: 0.6807\n",
      "Epoch [340/1000], Loss: 0.6802\n",
      "Epoch [350/1000], Loss: 0.6798\n",
      "Epoch [360/1000], Loss: 0.6793\n",
      "Epoch [370/1000], Loss: 0.6789\n",
      "Epoch [380/1000], Loss: 0.6784\n",
      "Epoch [390/1000], Loss: 0.6780\n",
      "Epoch [400/1000], Loss: 0.6775\n",
      "Epoch [410/1000], Loss: 0.6773\n",
      "Epoch [420/1000], Loss: 0.6775\n",
      "Epoch [430/1000], Loss: 0.6765\n",
      "Epoch [440/1000], Loss: 0.6761\n",
      "Epoch [450/1000], Loss: 0.6757\n",
      "Epoch [460/1000], Loss: 0.6753\n",
      "Epoch [470/1000], Loss: 0.6750\n",
      "Epoch [480/1000], Loss: 0.6749\n",
      "Epoch [490/1000], Loss: 0.6744\n",
      "Epoch [500/1000], Loss: 0.6743\n",
      "Epoch [510/1000], Loss: 0.6737\n",
      "Epoch [520/1000], Loss: 0.6734\n",
      "Epoch [530/1000], Loss: 0.6732\n",
      "Epoch [540/1000], Loss: 0.6728\n",
      "Epoch [550/1000], Loss: 0.6724\n",
      "Epoch [560/1000], Loss: 0.6721\n",
      "Epoch [570/1000], Loss: 0.6719\n",
      "Epoch [580/1000], Loss: 0.6716\n",
      "Epoch [590/1000], Loss: 0.6712\n",
      "Epoch [600/1000], Loss: 0.6712\n",
      "Epoch [610/1000], Loss: 0.6707\n",
      "Epoch [620/1000], Loss: 0.6704\n",
      "Epoch [630/1000], Loss: 0.6705\n",
      "Epoch [640/1000], Loss: 0.6699\n",
      "Epoch [650/1000], Loss: 0.6698\n",
      "Epoch [660/1000], Loss: 0.6696\n",
      "Epoch [670/1000], Loss: 0.6692\n",
      "Epoch [680/1000], Loss: 0.6695\n",
      "Epoch [690/1000], Loss: 0.6692\n",
      "Epoch [700/1000], Loss: 0.6689\n",
      "Epoch [710/1000], Loss: 0.6683\n",
      "Epoch [720/1000], Loss: 0.6682\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "model.train()  # Met le modèle en mode entraînement\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Affichage de la perte toutes les 10 époques\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Affichage final de la perte\n",
    "print(f\"Loss finale après {num_epochs} époques: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
